{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7076c6a3-41d8-4804-8551-86a06cc8f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -D dfs.replication=1 -cp -f data/*.jsonl hdfs://nn:9000/\n",
    "hdfs dfs -D dfs.replication=1 -cp -f data/*.csv hdfs://nn:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fe8f983-08b3-46c9-86a2-a51ea8c9af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"1G\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eddd6e7c-0e03-4fe3-9a55-60c085f2360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "|cf_contest_id|cf_index|cf_points|cf_rating|        cf_tags|difficulty|generated_tests|is_description_translated|memory_limit_bytes|                name|private_tests|problem_id|public_tests|source|time_limit|\n",
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "|          322|       A|    500.0|     1000|            [0]|         7|             93|                    false|         256000000|322_A. Ciel and D...|           45|         1|           2|     2|         1|\n",
      "|          760|       D|   1000.0|     1600|         [1, 2]|        10|             51|                    false|         256000000|  760_D. Travel Card|            4|         2|           2|     2|         2|\n",
      "|          569|       E|   1500.0|     2600|         [3, 0]|        11|             99|                    false|         256000000| 569_E. New Language|           17|         3|           3|     2|         2|\n",
      "|          447|       B|   1000.0|     1000|         [0, 4]|         8|            100|                    false|         256000000|447_B. DZY Loves ...|           13|         4|           1|     2|         1|\n",
      "|         1292|       B|    750.0|     1700|[5, 6, 7, 0, 4]|         8|             91|                    false|         256000000|1292_B. Aroma's S...|          131|         5|           3|     2|         1|\n",
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problems_df = spark.read.json(\"hdfs://nn:9000/problems.jsonl\")\n",
    "\n",
    "problems_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f980fe3-6540-4b52-a450-a94d21066c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"1G\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d3a9ee2-5c1b-476b-8870-fb0fbeab0081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "|cf_contest_id|cf_index|cf_points|cf_rating|        cf_tags|difficulty|generated_tests|is_description_translated|memory_limit_bytes|                name|private_tests|problem_id|public_tests|source|time_limit|\n",
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "|          322|       A|    500.0|     1000|            [0]|         7|             93|                    false|         256000000|322_A. Ciel and D...|           45|         1|           2|     2|         1|\n",
      "|          760|       D|   1000.0|     1600|         [1, 2]|        10|             51|                    false|         256000000|  760_D. Travel Card|            4|         2|           2|     2|         2|\n",
      "|          569|       E|   1500.0|     2600|         [3, 0]|        11|             99|                    false|         256000000| 569_E. New Language|           17|         3|           3|     2|         2|\n",
      "|          447|       B|   1000.0|     1000|         [0, 4]|         8|            100|                    false|         256000000|447_B. DZY Loves ...|           13|         4|           1|     2|         1|\n",
      "|         1292|       B|    750.0|     1700|[5, 6, 7, 0, 4]|         8|             91|                    false|         256000000|1292_B. Aroma's S...|          131|         5|           3|     2|         1|\n",
      "+-------------+--------+---------+---------+---------------+----------+---------------+-------------------------+------------------+--------------------+-------------+----------+------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problems_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "965998f8-23ef-4794-9e37-5520f48275f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "problems_rdd = problems_df.rdd\n",
    "\n",
    "filtered_rdd = problems_rdd.filter(lambda row: \n",
    "    row.cf_rating is not None and row.cf_rating >= 1600 and\n",
    "    row.private_tests is not None and row.private_tests > 0 and\n",
    "    row.name is not None and \"_A.\" in row.name\n",
    ")\n",
    "\n",
    "filtered_count = filtered_rdd.count()\n",
    "filtered_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8da2b3e-0b68-47d5-9462-a6e2306dda4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "from pyspark.sql.functions import expr\n",
    "filtered_df = problems_df.filter(expr(\"cf_rating >= 1600 AND private_tests > 0 AND name LIKE '%_A.%'\"))\n",
    "filtered_count_df = filtered_df.count()\n",
    "filtered_count_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dda72d09-1dbd-423d-bf9d-3ae405e6d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3 \n",
    "problems_df.write.mode(\"overwrite\").saveAsTable(\"problems_table\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS count\n",
    "    FROM problems_table\n",
    "    WHERE name LIKE '%_A.%'\n",
    "      AND cf_rating >= 1600\n",
    "      AND private_tests > 0\n",
    "\"\"\")\n",
    "result.first()['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6059245-946e-495a-82e7-7cdcb1f46f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+--------------------+\n",
      "|is_correct|language|problem_id|            solution|\n",
      "+----------+--------+----------+--------------------+\n",
      "|      true| PYTHON2|         1|n,m = [int(nm) fo...|\n",
      "|      true| PYTHON2|         1|mn = map(int, raw...|\n",
      "|      true| PYTHON2|         1|n,m=map(int, raw_...|\n",
      "|      true|    JAVA|         1|import java.io.Bu...|\n",
      "|      true|    JAVA|         1|import java.util....|\n",
      "+----------+--------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|      problems_table|      false|\n",
      "|  default|           solutions|      false|\n",
      "|  default|solutions_buckete...|      false|\n",
      "|         |           languages|       true|\n",
      "|         |       problem_tests|       true|\n",
      "|         |             sources|       true|\n",
      "|         |                tags|       true|\n",
      "+---------+--------------------+-----------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\\nAdaptiveSparkPlan (4)\\n+- HashAggregate (3)\\n   +- HashAggregate (2)\\n      +- Scan parquet spark_catalog.default.solutions_bucketed_parquet (1)\\n\\n\\n(1) Scan parquet spark_catalog.default.solutions_bucketed_parquet\\nOutput [1]: [language#3032]\\nBatched: true\\nBucketed: true\\nLocation: InMemoryFileIndex [hdfs://nn:9000/user/hive/warehouse/solutions_bucketed_parquet]\\nReadSchema: struct<language:string>\\nSelectedBucketsCount: 4 out of 4\\n\\n(2) HashAggregate\\nInput [1]: [language#3032]\\nKeys [1]: [language#3032]\\nFunctions [1]: [partial_count(1)]\\nAggregate Attributes [1]: [count#3037L]\\nResults [2]: [language#3032, count#3038L]\\n\\n(3) HashAggregate\\nInput [2]: [language#3032, count#3038L]\\nKeys [1]: [language#3032]\\nFunctions [1]: [count(1)]\\nAggregate Attributes [1]: [count(1)#3030L]\\nResults [2]: [language#3032, count(1)#3030L AS count(1)#3035L]\\n\\n(4) AdaptiveSparkPlan\\nOutput [2]: [language#3032, count(1)#3035L]\\nArguments: isFinalPlan=false\\n\\n|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q4\n",
    "\n",
    "solutions_df = spark.read.json(\"hdfs://nn:9000/solutions.jsonl\")\n",
    "solutions_df.show(5)  \n",
    "\n",
    "(solutions_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")  \n",
    "    .bucketBy(4, \"language\") \n",
    "    .sortBy(\"language\")  \n",
    "    .saveAsTable(\"solutions_bucketed_parquet\"))\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "query_plan = spark.sql(\"\"\"\n",
    "    EXPLAIN FORMATTED\n",
    "    SELECT language, COUNT(*)\n",
    "    FROM solutions_bucketed_parquet\n",
    "    GROUP BY language\n",
    "\"\"\")\n",
    "\n",
    "query_plan.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1c3943a5-f4ff-4ef0-aaad-8d487b35135b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problems': False,\n",
       " 'solutions': False,\n",
       " 'languages': True,\n",
       " 'problem_tests': True,\n",
       " 'sources': True,\n",
       " 'tags': True}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5 \n",
    "languages_df = spark.read.csv(\"hdfs://nn:9000/languages.csv\", header=True)\n",
    "languages_df.createOrReplaceTempView(\"languages\")\n",
    "problem_tests_df = spark.read.csv(\"hdfs://nn:9000/problem_tests.csv\", header=True)\n",
    "problem_tests_df.createOrReplaceTempView(\"problem_tests\")\n",
    "sources_df = spark.read.csv(\"hdfs://nn:9000/sources.csv\", header=True)\n",
    "sources_df.createOrReplaceTempView(\"sources\")\n",
    "tags_df = spark.read.csv(\"hdfs://nn:9000/tags.csv\", header=True)\n",
    "tags_df.createOrReplaceTempView(\"tags\")\n",
    "\n",
    "q5_answer = {\n",
    "    'problems': False,  \n",
    "    'solutions': False, \n",
    "    'languages': True,  \n",
    "    'problem_tests': True,  \n",
    "    'sources': True,  \n",
    "    'tags': True  \n",
    "}\n",
    "\n",
    "q5_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "199efc60-bc3e-41ca-af69-423e3490c9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "10576"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "solutions_df.write.mode(\"overwrite\").saveAsTable(\"solutions\")\n",
    "codeforces_id_df = spark.sql(\"SELECT source FROM sources WHERE source_name = 'CODEFORCES'\")\n",
    "codeforces_id = codeforces_id_df.collect()[0][0]\n",
    "\n",
    "q6_result = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count\n",
    "    FROM solutions s\n",
    "    JOIN problems_table p ON s.problem_id = p.problem_id\n",
    "    WHERE s.language = 'PYTHON3'\n",
    "    AND s.is_correct = TRUE\n",
    "    AND p.source = {codeforces_id}\n",
    "\"\"\")\n",
    "\n",
    "count = q6_result.collect()[0][0]\n",
    "count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "216b4a7e-6a27-49c6-ba3e-f43f62547dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Easy': 409, 'Medium': 5768, 'Hard': 2396}\n"
     ]
    }
   ],
   "source": [
    "#q7\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "problems_df = problems_df.withColumn(\n",
    "    \"difficulty_category\",\n",
    "    when(col(\"difficulty\") <= 5, \"Easy\").when(col(\"difficulty\") <= 10, \"Medium\").otherwise(\"Hard\"),\n",
    ")\n",
    "\n",
    "q7_result = problems_df.groupBy(\"difficulty_category\").count()\n",
    "\n",
    "q7_dict = {row.difficulty_category: row[\"count\"] for row in q7_result.collect()}\n",
    "\n",
    "print(q7_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0aa4466-ebe2-4761-b6de-af4ba9f3eb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3829801082611084, 0.5719237327575684, 0.160003662109375]\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "\n",
    "problem_tests = spark.table(\"problem_tests\") \n",
    "\n",
    "filtered_problem_tests = problem_tests.filter(F.col(\"is_generated\") == False)\n",
    "\n",
    "def compute_averages(df):\n",
    "    start_time = time.time()\n",
    "    averages = df.select(F.avg(\"input_chars\"), F.avg(\"output_chars\")).collect()[0]\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "times = []\n",
    "\n",
    "times.append(compute_averages(filtered_problem_tests))\n",
    "\n",
    "filtered_problem_tests.cache()\n",
    "\n",
    "times.append(compute_averages(filtered_problem_tests))\n",
    "\n",
    "times.append(compute_averages(filtered_problem_tests))\n",
    "\n",
    "filtered_problem_tests.unpersist()\n",
    "\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f36fef3b-beac-42d2-89e7-94ddbfb1dfb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/01 16:46:54 WARN CacheManager: Asked to cache already cached data.\n",
      "25/04/01 16:46:54 WARN CacheManager: Asked to cache already cached data.\n",
      "25/04/01 16:46:54 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction column exists.\n",
      "R² value: 0.5929835263198762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5929835263198762"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "codeforces_problems = problems_df.filter(F.col(\"source\") == 2).cache()\n",
    "\n",
    "train_df = codeforces_problems.filter(\n",
    "    (F.col(\"cf_rating\") > 0) & \n",
    "    (F.col(\"problem_id\") % 2 == 0) & \n",
    "    F.col(\"difficulty\").isNotNull() &\n",
    "    F.col(\"time_limit\").isNotNull() &\n",
    "    F.col(\"memory_limit_bytes\").isNotNull()\n",
    ").cache()\n",
    "\n",
    "test_df = codeforces_problems.filter(\n",
    "    (F.col(\"cf_rating\") > 0) & \n",
    "    (F.col(\"problem_id\") % 2 != 0) & \n",
    "    F.col(\"difficulty\").isNotNull() &\n",
    "    F.col(\"time_limit\").isNotNull() &\n",
    "    F.col(\"memory_limit_bytes\").isNotNull()\n",
    ").cache()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"difficulty\", \"time_limit\", \"memory_limit_bytes\"], outputCol=\"features\")\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"cf_rating\", maxDepth=5)\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "if \"prediction\" in predictions.columns:\n",
    "    print(\"Prediction column exists.\")\n",
    "else:\n",
    "    print(\"Prediction column does not exist.\")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"cf_rating\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "\n",
    "r2_float = float(r2)\n",
    "print(f\"R² value: {r2_float}\")  \n",
    "r2_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1efc966c-51ba-4a93-a500-e9b398c4121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1887.9377431906614, 1893.1106471816283, 1950.4728638818783)\n"
     ]
    }
   ],
   "source": [
    "#q10\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "codeforces_problems = problems_df.filter(F.col(\"source\") == 2)  \n",
    "\n",
    "train_df = codeforces_problems.filter((F.col(\"cf_rating\") > 0) & (F.col(\"problem_id\") % 2 == 0))\n",
    "test_df = codeforces_problems.filter((F.col(\"cf_rating\") > 0) & (F.col(\"problem_id\") % 2 != 0))\n",
    "missing_df = codeforces_problems.filter(F.col(\"cf_rating\") == 0)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"difficulty\", \"time_limit\", \"memory_limit_bytes\"], outputCol=\"features\")\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"cf_rating\", maxDepth=5)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "predictions_missing = model.transform(missing_df)\n",
    "\n",
    "avg_train = train_df.agg(F.avg(\"cf_rating\")).collect()[0][0]\n",
    "avg_test = test_df.agg(F.avg(\"cf_rating\")).collect()[0][0]\n",
    "avg_missing_predictions = predictions_missing.agg(F.avg(\"prediction\")).collect()[0][0]\n",
    "\n",
    "print((avg_train, avg_test, avg_missing_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a80ad-624c-4feb-9173-f0d7197dbafc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
